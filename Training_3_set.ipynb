{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "from torch import backends\n",
    "from beautifultable import BeautifulTable\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##SETTINGS\n",
    "doTrain = False\n",
    "doEval = False\n",
    "doExtractBaso = True\n",
    "\n",
    "nfold = 1 #number of folds to train\n",
    "fold_offset = 23\n",
    "lr=0.01 #learning rate\n",
    "\n",
    "batch_size = 32\n",
    "val_split = 0.2 #trainset percentage allocated for devset\n",
    "test_val_split = 0.1 #trainset percentage allocated for test_val set (i.e. the test set of known patients)\n",
    "\n",
    "#cwd = os.getcwd()\n",
    "#cwd = \"subjects/min-max/windows_20/tr-False_sliding_1_c-False/folds_inter_no_4_24_25/\"\n",
    "#cwd = \"subjects/min-max/clean/windows_20-del_tr-True-slide-False-digits-3/folds_inter/\"\n",
    "cwd = \"subjects/bpf-20-450_rect/windows_200-del_tr-False-slide-True-digits-3-pace-20/folds_inter/\"\n",
    "#cwd = \"subjects/min-max/clean/windows_200-del_tr-False-slide-True-digits-3-pace-1/folds_inter/\"\n",
    "#cwd = \"subjects/min-max/clean/windows_200-del_tr-False-slide-True-digits-3-pace-20/folds_inter/\"\n",
    "#cwd = \"subjects/min-max/clean/windows_200-del_tr-True-slide-False-digits-3/folds_inter/\"\n",
    "\n",
    "\n",
    "subject = 1 # serve per caricare le folds da cartelle diverse\n",
    "prefix_train = 'TrainFold'\n",
    "prefix_test = 'TestFold'\n",
    "\n",
    "#spw=20 #samples per window\n",
    "#nmuscles=10 #initial number of muscles acquired\n",
    "\n",
    "spw=1 #samples per window\n",
    "nmuscles=40 #initial number of muscles acquired\n",
    "\n",
    "#Enable/Disable shuffle on trainset/testset\n",
    "shuffle_train = False\n",
    "shuffle_test= False\n",
    "\n",
    "#Delete electrogonio signals\n",
    "# 3 = Left Rectus Femuris; 5 = Left Goniometer; 9 = Right Rectus Femuris; 11 = Right Goniometer\n",
    "exclude_features = True\n",
    "#Only use electrogonio signals\n",
    "include_only_features = False\n",
    "#Features to selected/deselected for input to the networks\n",
    "features_select = [] #1 to 4\n",
    "#features_select = [9, 10] #1 to 4\n",
    "#features_select = [3, 4, 7, 8, 9, 10] #1 to 4\n",
    "\n",
    "#Select which models to run. Insert comma separated values into 'model_select' var.\n",
    "#List. 0:'FF', 1:'FC2', 2:'FC2DP', 3:'FC3', 4:'FC3dp', 5:'Conv1d', 6:'MultiConv1d' \n",
    "#e.g: model_select = [0,4,6] to select FF,FC3dp,MultiConv1d\n",
    "\n",
    "# Modelli del paper: 11 (FF2), 14 (FF4), 16 (FF5) --> Prova questi!\n",
    "# FF6 per testarlo potente dopo (17)\n",
    "model_lst = ['FF','FC2','FC2DP','FC3','FC3dp','Conv1d','MultiConv1d',\n",
    "             'MultiConv1d_2','MultiConv1d_3', 'MultiConv1d_4', 'MultiConv1d_5', \n",
    "             'FF2', 'CNN1', 'FF3', 'FF4', 'CNN2', 'FF5', 'FF6', 'CNN3', 'CNN1-FF5', \n",
    "             'CNN1-2','CNN1-1', 'CNN1-3', 'CNN_w60', 'FF7', 'LSTM1', 'BAS-10', \"BAS-40\"]\n",
    "model_select = [17] \n",
    "\n",
    "#Early stop settings\n",
    "maxepoch = 100\n",
    "maxpatience = 10\n",
    "\n",
    "use_cuda = False\n",
    "use_gputil = False\n",
    "cuda_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CUDA\n",
    "\n",
    "if use_gputil and torch.cuda.is_available():\n",
    "    import GPUtil\n",
    "\n",
    "    # Get the first available GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    try:\n",
    "        deviceIDs = GPUtil.getAvailable(order='memory', limit=1, maxLoad=100, maxMemory=20)  # return a list of available gpus\n",
    "    except:\n",
    "        print('GPU not compatible with NVIDIA-SMI')\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(deviceIDs[0])\n",
    "        \n",
    "    ttens = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "    ttens = ttens.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? --> False\n",
      "Cuda Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print('Is CUDA available? --> ' + str(torch.cuda.is_available()))\n",
    "print('Cuda Device: ' + str(cuda_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seeds\n",
    "def setSeeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "setSeeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prints header of beautifultable report for each fold\n",
    "def header(model_list,nmodel,nfold,traindataset,testdataset, testdataset_U):\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('MODEL: '+model_list[nmodel])\n",
    "    print('Fold: '+str(nfold))\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++\\n\\n')\n",
    "    shape = list(traindataset.x_data.shape)\n",
    "    print('Trainset fold'+str(i)+' shape: '+str(shape[0])+'x'+str((shape[1]+1)))\n",
    "    shape = list(testdataset.x_data.shape)\n",
    "    print('Testset (L) fold'+str(i)+' shape: '+str(shape[0])+'x'+str((shape[1]+1)))\n",
    "    shape = list(testdataset_U.x_data.shape)\n",
    "    print('Testset (U) fold' + str(i) + ' shape: ' + str(shape[0])+ 'x' + str((shape[1]+1)) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prints actual beautifultable for each fold\n",
    "def table(model_list,nmodel,accuracies,precisions,recalls,f1_scores,accuracies_dev):\n",
    "    table = BeautifulTable()\n",
    "    table.column_headers = [\"{}\".format(model_list[nmodel]), \"Avg\", \"Stdev\"]\n",
    "    table.append_row([\"Accuracy\",round(np.average(accuracies),3),round(np.std(accuracies),3)])\n",
    "    table.append_row([\"Precision\",round(np.average(precisions),3),round(np.std(precisions),3)])\n",
    "    table.append_row([\"Recall\",round(np.average(recalls),3),round(np.std(recalls),3)])\n",
    "    table.append_row([\"F1_score\",round(np.average(f1_scores),3),round(np.std(f1_scores),3)])\n",
    "    table.append_row([\"Accuracy_dev\",round(np.average(accuracies_dev),3),round(np.std(accuracies_dev),3)])    \n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saves best model state on disk for each fold\n",
    "def save_checkpoint (state, is_best, filename, logfile):\n",
    "    if is_best:\n",
    "        msg = \"=> Saving a new best. \"+'Epoch: '+str(state['epoch'])\n",
    "        print (msg)\n",
    "        logfile.write(msg + \"\\n\")\n",
    "        torch.save(state, filename)  \n",
    "    else:\n",
    "        msg = \"=> Validation accuracy did not improve. \"+'Epoch: '+str(state['epoch'])\n",
    "        print (msg)\n",
    "        logfile.write(msg + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute sklearn metrics: Recall, Precision, F1-score\n",
    "def pre_rec (loader, model, positiveLabel):\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate (loader,0):\n",
    "            inputs, labels = data\n",
    "            y_true = np.append(y_true,labels.cpu())\n",
    "            outputs = model(inputs)\n",
    "            outputs[outputs>=0.5] = 1\n",
    "            outputs[outputs<0.5] = 0\n",
    "            y_pred = np.append(y_pred,outputs.cpu())\n",
    "    y_true = np.where(y_true==positiveLabel,0,1)\n",
    "    y_pred = np.where(y_pred==positiveLabel,0,1)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    return round(precision*100,3), round(recall*100,3), round(f1_score*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates model accuracy. Predicted vs Correct.\n",
    "def accuracy (loader, model):\n",
    "    total=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            inputs, labels = data    # Carica i dati\n",
    "            outputs = model(inputs)    # Tira fuori gli output\n",
    "            outputs[outputs>=0.5] = 1    # Se l'output è >= 0.5 --> outputs[i] = 1\n",
    "            outputs[outputs<0.5] = 0    # Se l'output è < 0.5 --> outputs[i] = 0\n",
    "            total += labels.size(0)     # Calcola i labels totali\n",
    "            correct += (outputs == labels).sum().item()    # Conta i corretti (quelli che matchano)\n",
    "    return round((100 * correct / total),3)    # Tira fuori la percentuale di accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Arrays to store metrics\n",
    "accs = np.empty([nfold,1])\n",
    "accs_test_val = np.empty([nfold,1])\n",
    "precisions_0_U = np.empty([nfold,1])\n",
    "recalls_0_U = np.empty([nfold,1])\n",
    "f1_scores_0_U = np.empty([nfold,1])\n",
    "precisions_1_U = np.empty([nfold,1])\n",
    "recalls_1_U = np.empty([nfold,1])\n",
    "f1_scores_1_U = np.empty([nfold,1])\n",
    "precisions_0_L = np.empty([nfold,1])\n",
    "recalls_0_L = np.empty([nfold,1])\n",
    "f1_scores_0_L = np.empty([nfold,1])\n",
    "precisions_1_L = np.empty([nfold,1])\n",
    "recalls_1_L = np.empty([nfold,1])\n",
    "f1_scores_1_L = np.empty([nfold,1])\n",
    "accs_dev = np.empty([nfold,1])\n",
    "times = np.empty([nfold,1])\n",
    "\n",
    "#Calculate avg metrics on folds\n",
    "def averages (vals):\n",
    "    avgs = []\n",
    "    for val in vals:\n",
    "        avgs.append(round(np.average(val),3))\n",
    "    return avgs\n",
    "\n",
    "#Calculate std metrics on folds\n",
    "def stds (vals):\n",
    "    stds = []\n",
    "    for val in vals:\n",
    "        stds.append(round(np.std(val),3))\n",
    "    return stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Salva il basografico predetto su un file sfruttando l'accuracy\n",
    "\n",
    "def save_predicted_baso_naive(loader, model, subject_predict, model_select, num_fold):\n",
    "    print(\"predicting baso for subject \" + str(subject_predict) + \" ... \")\n",
    "    \n",
    "    print(\"loader: \" + str(len(loader)))\n",
    "    \n",
    "    windows_length = 20\n",
    "    predicted_baso_windowed = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ## -- Estrae gli output di ciascuna window\n",
    "        for i, data, in enumerate(loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            outputs[outputs >= 0.5] = 1\n",
    "            outputs[outputs < 0.5] = 0\n",
    "            predicted_temp = outputs.tolist()\n",
    "            predicted_baso_windowed.extend(predicted_temp)\n",
    "        \n",
    "        print(\"predicted_baso_windowed: \" + str(len(predicted_baso_windowed)))\n",
    "        \n",
    "        predicted_baso_windowed = pd.DataFrame(predicted_baso_windowed)    # Converto in DataFrame\n",
    "        predicted_baso_windowed = predicted_baso_windowed.as_matrix()\n",
    "        \n",
    "        limit = len(predicted_baso_windowed)\n",
    "        predicted_baso = np.empty([limit*windows_length,1], dtype = int)\n",
    "        \n",
    "        count = 0\n",
    "        for j in range(0, limit):\n",
    "            predicted_baso[count:count+windows_length] = predicted_baso_windowed[j]\n",
    "            count += windows_length\n",
    "            \n",
    "        predicted_baso = pd.DataFrame(predicted_baso)\n",
    "        out_path = 'subjects/min-max/clean/windows_20-del_tr-True-slide-False-digits-3/folds_inter/Report_' + str(model_select) + '/Fold_' + str(num_fold) + '/' \n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        #np.savetxt(out_path + 's' + str(subject_predict) + '_predicted.csv', predicted_baso, delimiter = \",\")\n",
    "        predicted_baso = predicted_baso.to_csv(out_path + 's' + str(subject_predict) + '_predicted.csv', index = None, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## -- Salva il basografico predetto su un file sfruttando l'accuracy\n",
    "\n",
    "def save_predicted_baso(loader, model, subject_predict, model_select, num_fold):\n",
    "    print(\"predicting baso for subject \" + str(subject_predict) + \" ... \")\n",
    "    #windows_length = 20\n",
    "    windows_length = 200\n",
    "    pace = 20\n",
    "    \n",
    "    predicted_baso_windowed = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ## -- Estrae gli output di ciascuna window\n",
    "        for i, data, in enumerate(loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            outputs[outputs >= 0.5] = 1\n",
    "            outputs[outputs < 0.5] = 0\n",
    "            predicted_temp = outputs.tolist()\n",
    "            predicted_baso_windowed.extend(predicted_temp)\n",
    "        \n",
    "        predicted_baso_windowed = pd.DataFrame(predicted_baso_windowed)    # Converto in DataFrame\n",
    "        predicted_baso_windowed = predicted_baso_windowed.as_matrix()\n",
    "        \n",
    "        print(\"NN predicitons on sliding wondows: \")\n",
    "        print(predicted_baso_windowed)\n",
    "        limit = len(predicted_baso_windowed)\n",
    "        number_of_samples = (limit-1)*pace+windows_length\n",
    "        print(\"Number of sliding windows: \" + str(limit))\n",
    "        print(\"Number of samples in a window: \" + str(windows_length))\n",
    "        print(\"Number of samples: \" + str(number_of_samples))\n",
    "        \n",
    "        samples_predictions = np.zeros(number_of_samples, dtype = int)\n",
    "\n",
    "        for w in range(0,limit):\n",
    "            if (predicted_baso_windowed[w]==0):\n",
    "                add = -1\n",
    "            elif (predicted_baso_windowed[w]==1):\n",
    "                add = 1    \n",
    "            for s in range(w*pace, w*pace + windows_length):\n",
    "                samples_predictions[s] += add\n",
    "\n",
    "        print(\"\\nSamples predictions:\")\n",
    "        print(samples_predictions)\n",
    "\n",
    "        #Per ogni campione la predizione vincente sarà il massimo tra gli 1 e gli 0 predetti\n",
    "        predicted_baso = np.zeros(number_of_samples, dtype = int)\n",
    "        count = 0\n",
    "        for sample_pred in samples_predictions:\n",
    "            if (sample_pred > 0):\n",
    "                predicted_baso[count] = 1\n",
    "            elif (sample_pred < 0): \n",
    "                predicted_baso[count] = 0\n",
    "            else:\n",
    "                predicted_baso[count] = predicted_baso[count-1]\n",
    "            #print(\"0:\" + str(zeros), \"1:\" + str(ones), str(predicted_baso[count]))\n",
    "            count += 1\n",
    "\n",
    "            \n",
    "        predicted_baso = pd.DataFrame(predicted_baso)\n",
    "        #out_path = 'subjects/min-max/windows_20-del_tr-False-slide-False-digits-4/folds_inter/Report_' + str(model_select) + '/Fold_' + str(num_fold) + '/' \n",
    "        #out_path = 'subjects/min-max/clean/windows_20-del_tr-False-slide-True-digits-3-pace-1/folds_inter/Report_' + str(model_select) + '/Fold_' + str(num_fold) + '/' \n",
    "        #out_path = 'subjects/min-max/clean/windows_200-del_tr-False-slide-True-digits-3-pace-20/folds_inter/Report_' + str(model_select) + '/Fold_' + str(num_fold) + '/' \n",
    "        out_path = 'subjects/bpf-20-450_rect/windows_200-del_tr-False-slide-True-digits-3-pace-20/folds_inter/Report_' + str(model_select) + '/Fold_' + str(num_fold) + '/' \n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        #np.savetxt(out_path + 's' + str(subject_predict) + '_predicted.csv', predicted_baso, delimiter = \",\")\n",
    "        predicted_baso = predicted_baso.to_csv(out_path + 's' + str(subject_predict) + '_predicted.csv', index = None, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle\n",
    "def dev_shuffle (shuffle_train,shuffle_test,val_split,traindataset,testdataset):\n",
    "    train_size = len(traindataset)\n",
    "    test_size = len(testdataset)\n",
    "    train_indices = list(range(train_size))\n",
    "    test_indices = list(range(test_size))\n",
    "    split = int(np.floor(val_split * train_size))\n",
    "    if shuffle_train:\n",
    "        np.random.shuffle(train_indices)\n",
    "    if shuffle_test:\n",
    "        np.random.shuffle(test_indices) \n",
    "    train_indices, dev_indices = train_indices[split:], train_indices[:split]\n",
    "    # Samplers\n",
    "    tr_sampler = SubsetRandomSampler(train_indices)\n",
    "    d_sampler = SubsetRandomSampler(dev_indices)\n",
    "    te_sampler = SubsetRandomSampler(test_indices)\n",
    "    return tr_sampler,d_sampler,te_sampler\n",
    "\n",
    "# Questa funzione serve dividere i vari set una volta caricate le fold\n",
    "# In particolare, verrà usata solo per 'dev_loader', dato che lo split \n",
    "# nei vari set è già stato fatto quando sono state create le fold\n",
    "def data_split (shuffle_train,shuffle_test,val_split,test_val_split,traindataset,testdataset):\n",
    "    train_size = len(traindataset)\n",
    "    test_size = len(testdataset)\n",
    "    train_indices = list(range(train_size))\n",
    "    test_indices = list(range(test_size))\n",
    "    test_val_split = int(np.floor(test_val_split * train_size)) \n",
    "    dev_split = int(np.floor(val_split * (train_size-test_val_split) + test_val_split))\n",
    "    if shuffle_train:\n",
    "        np.random.shuffle(train_indices)\n",
    "    if shuffle_test:\n",
    "        np.random.shuffle(test_indices) \n",
    "    train_indices, dev_indices, test_val_indices = train_indices[dev_split:], train_indices[test_val_split:dev_split], train_indices[:test_val_split]\n",
    "    # Samplers\n",
    "    tr_sampler = SubsetRandomSampler(train_indices)\n",
    "    d_sampler = SubsetRandomSampler(dev_indices)\n",
    "    tv_sampler = SubsetRandomSampler(test_val_indices)                \n",
    "    te_sampler = SubsetRandomSampler(test_indices)\n",
    "    return tr_sampler,d_sampler,tv_sampler,te_sampler\n",
    "\n",
    "# Questa funzione serve se usiamo come input della rete 3 files:\n",
    "    # TRAINFOLD\n",
    "    # TESTFOLD_LEARNED\n",
    "    # TESTFOLD_UNLEARNED\n",
    "# L'unico file che deve essere effettivamente suddiviso, in questo caso, è il TRAINFOLD, dato che useremo\n",
    "# una percentuale 'val_split' (hyperparameter) per il validation, e una percentuale 1-'val_split'\n",
    "# per il vero e proprio training della rete.\n",
    "# La funzione da in output i sampler da utilizzare come parametri per il DataLoader:\n",
    "    # train_sampler\n",
    "    # dev_sampler\n",
    "def split_train ( val_split, traindataset ): \n",
    "    train_size = len(traindataset)    # lunghezza del TRAIN_DATASET\n",
    "    train_indices = list(range(train_size))    # crea una lista che va da 0 -> lunghezza di train_size totale\n",
    "    dev_size = int(np.floor(val_split * train_size))    # Dimensione del dev_set\n",
    "    split = int(np.floor((train_size - dev_size)))    # Valore di split dei 2 set\n",
    "\n",
    "    train_indices_bis, dev_indices = train_indices[:split], train_indices[split:]    # Indici splittati\n",
    "        \n",
    "    # Sampler\n",
    "    tr_sampler = SubsetRandomSampler(train_indices_bis)    # Train Sampler\n",
    "    d_sampler = SubsetRandomSampler(dev_indices)    # Dev Sampler\n",
    "    return tr_sampler, d_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianmorbidoni/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py:1250: ResourceWarning: unclosed file <_io.TextIOWrapper name='subjects/bpf-20-450_rect/windows_200-del_tr-False-slide-True-digits-3-pace-20/folds_inter/Report_FF6/log.txt' mode='w' encoding='UTF-8'>\n",
      "  infer_datetime_format=self.infer_datetime_format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 458180 windows;\n",
      "Test dataset (LEARNED): 50943 windows;\n",
      "Test dataset (UNLEARNED) 13583 windows.\n",
      "\n",
      "Each window is composed by 41 samples.\n",
      "The number of muscles is 40\n"
     ]
    }
   ],
   "source": [
    "#Loads and appends all folds all at once\n",
    "trainfolds = []    # Train set\n",
    "testfolds_L = []    # Test set (LEARNED)\n",
    "testfolds_U = []    # Test set (UNLEARNED)\n",
    "\n",
    "col_select = np.array([])\n",
    "\n",
    "#This is an hack to test smaller windows\n",
    "#for i in range (spw*nmuscles,200):\n",
    "#    col_select = np.append(col_select,i)\n",
    "    \n",
    "for i in range (0,spw*nmuscles,nmuscles):\n",
    "    for muscle in features_select:\n",
    "        col_select = np.append(col_select,muscle -1 + i)\n",
    "    cols=np.arange(0,spw*nmuscles+1)\n",
    "\n",
    "#print(\"col_select\", col_select)\n",
    "    \n",
    "if exclude_features & (not include_only_features): #delete gonio\n",
    "    for j in range(fold_offset,fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_csv(os.path.join(cwd, prefix_train + '_' + str(j)+'.csv'),sep=',',header=None,dtype=np.float32,usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        trainfolds.append(traindata)\n",
    "        testdata_L = pd.read_csv(os.path.join(cwd, prefix_test + '_L_' + str(j)+'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        testfolds_L.append(testdata_L) \n",
    "        testdata_U = pd.read_csv(os.path.join(cwd, prefix_test + '_U_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        testfolds_U.append(testdata_U)\n",
    "        print('Train dataset: ' + str(len(traindata)) + ' windows;')\n",
    "        print('Test dataset (LEARNED): ' + str(len(testdata_L)) + ' windows;')\n",
    "        print('Test dataset (UNLEARNED) ' + str(len(testdata_U)) + ' windows.')\n",
    "        \n",
    "elif include_only_features & (not exclude_features): #only gonio\n",
    "    for j in range(fold_offset, fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_csv(os.path.join(cwd, prefix_train + '_' + str(j)+'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i in col_select.astype(int)])\n",
    "        testdata_L = pd.read_csv(os.path.join(cwd, prefix_test + '_L_'+ str(j)+'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i in col_select.astype(int)])\n",
    "        testdata_U = pd.read_csv(os.path.join(cwd, prefix_test + '_U_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i in col_select.astype(int)])\n",
    "        trainfolds.append(traindata)\n",
    "        testfolds_L.append(testdata_L)\n",
    "        testfolds_U.append(testdata_U)\n",
    "        print('Train dataset: ' + str(len(traindata)) + ' windows;')\n",
    "        print('Test dataset (LEARNED): ' + str(len(testdata)) + ' windows;')\n",
    "        print('Test dataset (UNLEARNED) ' + str(len(testdata_U)) + ' windows.')\n",
    "        \n",
    "elif (not include_only_features) & (not exclude_features): \n",
    "    for j in range(fold_offset,fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_csv(os.path.join(cwd, prefix_train + '_' + str(j) + '.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        testdata = pd.read_csv(os.path.join(cwd, prefix_test + '_L_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        testdata_U = pd.read_csv(os.path.join(cwd, prefix_test + '_U_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        trainfolds.append(traindata)\n",
    "        testfolds_L.append(testdata)\n",
    "        testfolds_U.append(testdata_U)    # Aggiunto testfold UNLEARNED \n",
    "        print('Train dataset: ' + str(len(traindata)) + ' windows;')\n",
    "        print('Test dataset (LEARNED): ' + str(len(testdata)) + ' windows;')\n",
    "        print('Test dataset (UNLEARNED) ' + str(len(testdata_U)) + ' windows.')\n",
    "else:\n",
    "    raise ValueError('use_gonio and del_gonio cannot be both True')\n",
    "\n",
    "nmuscles=int((len(traindata.columns)-1)/spw)\n",
    "print('\\nEach window is composed by ' + str(len(traindata.columns)) + ' samples.')\n",
    "print('The number of muscles is ' + str(nmuscles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.103253</td>\n",
       "      <td>0.106114</td>\n",
       "      <td>0.433873</td>\n",
       "      <td>0.115090</td>\n",
       "      <td>0.115205</td>\n",
       "      <td>0.052728</td>\n",
       "      <td>0.047042</td>\n",
       "      <td>0.195996</td>\n",
       "      <td>0.030849</td>\n",
       "      <td>0.036850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108914</td>\n",
       "      <td>0.434562</td>\n",
       "      <td>0.111994</td>\n",
       "      <td>0.112757</td>\n",
       "      <td>0.049992</td>\n",
       "      <td>0.045090</td>\n",
       "      <td>0.205394</td>\n",
       "      <td>0.033730</td>\n",
       "      <td>0.039396</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122079</td>\n",
       "      <td>0.124598</td>\n",
       "      <td>0.464238</td>\n",
       "      <td>0.131476</td>\n",
       "      <td>0.131965</td>\n",
       "      <td>0.049926</td>\n",
       "      <td>0.044172</td>\n",
       "      <td>0.179265</td>\n",
       "      <td>0.027742</td>\n",
       "      <td>0.033596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126469</td>\n",
       "      <td>0.469395</td>\n",
       "      <td>0.130744</td>\n",
       "      <td>0.131656</td>\n",
       "      <td>0.046971</td>\n",
       "      <td>0.041355</td>\n",
       "      <td>0.174570</td>\n",
       "      <td>0.027021</td>\n",
       "      <td>0.032810</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117345</td>\n",
       "      <td>0.119559</td>\n",
       "      <td>0.452092</td>\n",
       "      <td>0.125524</td>\n",
       "      <td>0.125513</td>\n",
       "      <td>0.049909</td>\n",
       "      <td>0.044095</td>\n",
       "      <td>0.177174</td>\n",
       "      <td>0.027398</td>\n",
       "      <td>0.033384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121973</td>\n",
       "      <td>0.458573</td>\n",
       "      <td>0.126222</td>\n",
       "      <td>0.127083</td>\n",
       "      <td>0.046906</td>\n",
       "      <td>0.041266</td>\n",
       "      <td>0.171014</td>\n",
       "      <td>0.026840</td>\n",
       "      <td>0.032724</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101164</td>\n",
       "      <td>0.101077</td>\n",
       "      <td>0.398111</td>\n",
       "      <td>0.099141</td>\n",
       "      <td>0.099750</td>\n",
       "      <td>0.056890</td>\n",
       "      <td>0.051831</td>\n",
       "      <td>0.213027</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>0.044577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105525</td>\n",
       "      <td>0.404126</td>\n",
       "      <td>0.100527</td>\n",
       "      <td>0.101267</td>\n",
       "      <td>0.054701</td>\n",
       "      <td>0.049917</td>\n",
       "      <td>0.209247</td>\n",
       "      <td>0.039001</td>\n",
       "      <td>0.044863</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.095277</td>\n",
       "      <td>0.094030</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.087886</td>\n",
       "      <td>0.086990</td>\n",
       "      <td>0.058765</td>\n",
       "      <td>0.054421</td>\n",
       "      <td>0.233642</td>\n",
       "      <td>0.044080</td>\n",
       "      <td>0.050065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095843</td>\n",
       "      <td>0.375042</td>\n",
       "      <td>0.086890</td>\n",
       "      <td>0.087344</td>\n",
       "      <td>0.057591</td>\n",
       "      <td>0.053598</td>\n",
       "      <td>0.234143</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>0.051329</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.083727</td>\n",
       "      <td>0.080450</td>\n",
       "      <td>0.316802</td>\n",
       "      <td>0.066789</td>\n",
       "      <td>0.066449</td>\n",
       "      <td>0.060210</td>\n",
       "      <td>0.055992</td>\n",
       "      <td>0.241111</td>\n",
       "      <td>0.046185</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082444</td>\n",
       "      <td>0.323301</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.067417</td>\n",
       "      <td>0.065061</td>\n",
       "      <td>0.060996</td>\n",
       "      <td>0.247481</td>\n",
       "      <td>0.052924</td>\n",
       "      <td>0.056314</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.078721</td>\n",
       "      <td>0.074442</td>\n",
       "      <td>0.288799</td>\n",
       "      <td>0.056885</td>\n",
       "      <td>0.057268</td>\n",
       "      <td>0.077012</td>\n",
       "      <td>0.071177</td>\n",
       "      <td>0.282044</td>\n",
       "      <td>0.053788</td>\n",
       "      <td>0.060250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077079</td>\n",
       "      <td>0.299628</td>\n",
       "      <td>0.059985</td>\n",
       "      <td>0.060308</td>\n",
       "      <td>0.073141</td>\n",
       "      <td>0.068144</td>\n",
       "      <td>0.282454</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.061959</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.074732</td>\n",
       "      <td>0.277328</td>\n",
       "      <td>0.054119</td>\n",
       "      <td>0.054509</td>\n",
       "      <td>0.068328</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.268599</td>\n",
       "      <td>0.049222</td>\n",
       "      <td>0.055602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079096</td>\n",
       "      <td>0.291173</td>\n",
       "      <td>0.057336</td>\n",
       "      <td>0.057906</td>\n",
       "      <td>0.071001</td>\n",
       "      <td>0.066236</td>\n",
       "      <td>0.269413</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.058160</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.084773</td>\n",
       "      <td>0.080224</td>\n",
       "      <td>0.294197</td>\n",
       "      <td>0.061042</td>\n",
       "      <td>0.061476</td>\n",
       "      <td>0.069127</td>\n",
       "      <td>0.064330</td>\n",
       "      <td>0.273977</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.058275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084272</td>\n",
       "      <td>0.306730</td>\n",
       "      <td>0.065358</td>\n",
       "      <td>0.065743</td>\n",
       "      <td>0.071430</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>0.280379</td>\n",
       "      <td>0.055996</td>\n",
       "      <td>0.061390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.084884</td>\n",
       "      <td>0.079816</td>\n",
       "      <td>0.284413</td>\n",
       "      <td>0.058330</td>\n",
       "      <td>0.058847</td>\n",
       "      <td>0.069545</td>\n",
       "      <td>0.064259</td>\n",
       "      <td>0.274873</td>\n",
       "      <td>0.049629</td>\n",
       "      <td>0.056023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083193</td>\n",
       "      <td>0.299966</td>\n",
       "      <td>0.063421</td>\n",
       "      <td>0.063805</td>\n",
       "      <td>0.071517</td>\n",
       "      <td>0.066279</td>\n",
       "      <td>0.276526</td>\n",
       "      <td>0.053148</td>\n",
       "      <td>0.059126</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.103253  0.106114  0.433873  0.115090  0.115205  0.052728  0.047042   \n",
       "1  0.122079  0.124598  0.464238  0.131476  0.131965  0.049926  0.044172   \n",
       "2  0.117345  0.119559  0.452092  0.125524  0.125513  0.049909  0.044095   \n",
       "3  0.101164  0.101077  0.398111  0.099141  0.099750  0.056890  0.051831   \n",
       "4  0.095277  0.094030  0.371795  0.087886  0.086990  0.058765  0.054421   \n",
       "5  0.083727  0.080450  0.316802  0.066789  0.066449  0.060210  0.055992   \n",
       "6  0.078721  0.074442  0.288799  0.056885  0.057268  0.077012  0.071177   \n",
       "7  0.079692  0.074732  0.277328  0.054119  0.054509  0.068328  0.063187   \n",
       "8  0.084773  0.080224  0.294197  0.061042  0.061476  0.069127  0.064330   \n",
       "9  0.084884  0.079816  0.284413  0.058330  0.058847  0.069545  0.064259   \n",
       "\n",
       "         7         8         9  ...         31        32        33        34  \\\n",
       "0  0.195996  0.030849  0.036850 ...   0.108914  0.434562  0.111994  0.112757   \n",
       "1  0.179265  0.027742  0.033596 ...   0.126469  0.469395  0.130744  0.131656   \n",
       "2  0.177174  0.027398  0.033384 ...   0.121973  0.458573  0.126222  0.127083   \n",
       "3  0.213027  0.038398  0.044577 ...   0.105525  0.404126  0.100527  0.101267   \n",
       "4  0.233642  0.044080  0.050065 ...   0.095843  0.375042  0.086890  0.087344   \n",
       "5  0.241111  0.046185  0.052274 ...   0.082444  0.323301  0.066900  0.067417   \n",
       "6  0.282044  0.053788  0.060250 ...   0.077079  0.299628  0.059985  0.060308   \n",
       "7  0.268599  0.049222  0.055602 ...   0.079096  0.291173  0.057336  0.057906   \n",
       "8  0.273977  0.051900  0.058275 ...   0.084272  0.306730  0.065358  0.065743   \n",
       "9  0.274873  0.049629  0.056023 ...   0.083193  0.299966  0.063421  0.063805   \n",
       "\n",
       "         35        36        37        38        39   40  \n",
       "0  0.049992  0.045090  0.205394  0.033730  0.039396  1.0  \n",
       "1  0.046971  0.041355  0.174570  0.027021  0.032810  1.0  \n",
       "2  0.046906  0.041266  0.171014  0.026840  0.032724  1.0  \n",
       "3  0.054701  0.049917  0.209247  0.039001  0.044863  1.0  \n",
       "4  0.057591  0.053598  0.234143  0.045902  0.051329  1.0  \n",
       "5  0.065061  0.060996  0.247481  0.052924  0.056314  1.0  \n",
       "6  0.073141  0.068144  0.282454  0.055999  0.061959  1.0  \n",
       "7  0.071001  0.066236  0.269413  0.055152  0.058160  1.0  \n",
       "8  0.071430  0.066744  0.280379  0.055996  0.061390  1.0  \n",
       "9  0.071517  0.066279  0.276526  0.053148  0.059126  1.0  \n",
       "\n",
       "[10 rows x 41 columns]"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Subject 26\n",
      "Subject 26 loaded: 13583 windows.\n",
      "\n",
      "Each window is composed by 41 samples.\n",
      "The number of muscles is 40\n"
     ]
    }
   ],
   "source": [
    "## -- Carica il soggetto su cui vuoi predire il basografico\n",
    "\n",
    "nmuscles = 10\n",
    "subject_predict = 26\n",
    "#directory_windows = '../subjects/min-max/windows_20/tr-False_sliding_1_c-False/'\n",
    "#directory_windows = 'subjects/min-max/clean/windows_20-del_tr-False-slide-True-digits-3-pace-1/'\n",
    "#directory_windows = 'subjects/min-max/clean/windows_20-del_tr-True-slide-False-digits-3/'\n",
    "#directory_windows = 'subjects/min-max/clean/windows_200-del_tr-False-slide-True-digits-3-pace-20/'\n",
    "directory_windows = 'subjects/bpf-20-450_rect/windows_200-del_tr-False-slide-True-digits-3-pace-20/'\n",
    "\n",
    "subj_prefix = 's_'\n",
    "#subj_suffix = '_norm_windows_20.csv'\n",
    "subj_suffix = '_windows_200_features_norm.csv'\n",
    "file = directory_windows + subj_prefix + str(subject_predict) + subj_suffix\n",
    "\n",
    "if doExtractBaso:\n",
    "    if exclude_features & (not include_only_features): #delete gonio\n",
    "        for j in range(fold_offset,fold_offset + nfold):\n",
    "            testfolds_U = []\n",
    "\n",
    "        for i in range(spw*nmuscles,200):\n",
    "            col_select = np.append(col_select,i)\n",
    "\n",
    "        for i in range (0,spw*nmuscles,nmuscles):\n",
    "            for muscle in features_select:\n",
    "                 col_select = np.append(col_select,muscle -1 + i)\n",
    "            cols=np.arange(0,spw*nmuscles+1)\n",
    "        \n",
    "#        cols = [i for i in range(0,2001)]\n",
    "        cols = [i for i in range(0,41)]\n",
    "        \n",
    "        col_select = []\n",
    "#        for i in range(0,200,nmuscles):\n",
    "#            col_select.append(8 + i)\n",
    "#            col_select.append(9 + i)\n",
    "        \n",
    "        cols = np.asarray(cols)\n",
    "        col_select = np.asarray(col_select)\n",
    "        \n",
    "        print(\"Loading Subject \" + str(subject_predict))\n",
    "        testdata_U = pd.read_csv(file,sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        testfolds_U.append(testdata_U)\n",
    "        #testdata_U = pd.read_csv(file,sep=',',header=None,dtype=np.float32)\n",
    "        #testfolds_U.append(testdata_U)\n",
    "        print('Subject ' + str(subject_predict) + ' loaded: ' + str(len(testdata_U)) + ' windows.')\n",
    "\n",
    "        nmuscles=int((len(traindata.columns)-1)/spw)\n",
    "        print('\\nEach window is composed by ' + str(len(testdata_U.columns)) + ' samples.')\n",
    "        print('The number of muscles is ' + str(nmuscles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmuscles=int((len(traindata.columns)-1)/spw) #used for layer dimensions and stride CNNs\n",
    "#trainfolds = None\n",
    "#traindata = None\n",
    "#import gc\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models\n",
    "from models import *\n",
    "models._spw = spw\n",
    "models._nmuscles = nmuscles\n",
    "models._batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(models._nmuscles)\n",
    "\n",
    "#import models\n",
    "#from models import *\n",
    "#TEST DIMENSIONS\n",
    "#models.nmuscles = nmuscles\n",
    "def testdimensions():\n",
    "    model = Model23()\n",
    "    print(model)\n",
    "    x = torch.randn(32,1,480)\n",
    "    model.test_dim(x)\n",
    " \n",
    "#testdimensions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = ['Fold','Acc_L', 'Acc_U',\n",
    "              'R_0_U','R_1_U',\n",
    "              'R_0_L','R_1_L',\n",
    "              'Stop_epoch','Accuracy_dev'] #coloumn names report FOLD CSV\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#TRAINING LOOP\n",
    "def train_test():\n",
    "    for k in model_select:\n",
    "        \n",
    "        table = BeautifulTable()\n",
    "        avgtable = BeautifulTable()\n",
    "        fieldnames1 = [model_lst[k],'Avg','Std_dev'] #column names report GLOBAL CSV\n",
    "        folder = os.path.join(cwd,'Report_'+str(model_lst[k]))\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        logfilepath = os.path.join(folder,'log.txt')\n",
    "        logfile = open(logfilepath,\"w\") \n",
    "\n",
    "        with open(os.path.join(folder,'Report_folds.csv'),'w') as f_fold, open(os.path.join(folder,'Report_global.csv'),'w') as f_global:\n",
    "            writer = csv.DictWriter(f_fold, fieldnames = fieldnames)\n",
    "            writer1  = csv.DictWriter(f_global, fieldnames = fieldnames1)\n",
    "            writer.writeheader()\n",
    "            writer1.writeheader()\n",
    "            t0 = 0\n",
    "            t1 = 0\n",
    "            for i in range(1,nfold+1):\n",
    "                \n",
    "                t0 = time.time()\n",
    "                setSeeds(0)\n",
    "                \n",
    "                class Traindataset(Dataset):\n",
    "                    def __init__(self):\n",
    "                        self.data=trainfolds[i-1]\n",
    "                        self.x_data=torch.from_numpy(np.asarray(self.data.iloc[:, 0:-1])) \n",
    "                        self.len=self.data.shape[0]\n",
    "                        self.y_data = torch.from_numpy(np.asarray(self.data.iloc[:, [-1]]))\n",
    "                        if (use_cuda):\n",
    "                            self.x_data = self.x_data.cuda()\n",
    "                            self.y_data = self.y_data.cuda()\n",
    "                    def __getitem__(self, index):\n",
    "                        return self.x_data[index], self.y_data[index]\n",
    "                    def __len__(self):\n",
    "                        return self.len\n",
    "                \n",
    "                # Classe per il Testset LEARNED\n",
    "                class Testdataset_L(Dataset):\n",
    "                    def __init__(self):\n",
    "                        self.data=testfolds_L[i-1]\n",
    "                        self.x_data=torch.from_numpy(np.asarray(self.data.iloc[:, 0:-1]))\n",
    "                        self.len=self.data.shape[0]\n",
    "                        self.y_data = torch.from_numpy(np.asarray(self.data.iloc[:, [-1]]))\n",
    "                        if (use_cuda):\n",
    "                            self.x_data = self.x_data.cuda()\n",
    "                            self.y_data = self.y_data.cuda()\n",
    "                    def __getitem__(self, index):\n",
    "                        return self.x_data[index], self.y_data[index]\n",
    "                    def __len__(self):\n",
    "                        return self.len\n",
    "                \n",
    "                # Classe per il Testset UNLEARNED\n",
    "                class Testdataset_U(Dataset):\n",
    "                    def __init__(self):\n",
    "                        self.data=testfolds_U[i-1]\n",
    "                        self.x_data=torch.from_numpy(np.asarray(self.data.iloc[:, 0:-1]))\n",
    "                        self.len=self.data.shape[0]\n",
    "                        self.y_data = torch.from_numpy(np.asarray(self.data.iloc[:, [-1]]))\n",
    "                        if (use_cuda):\n",
    "                            self.x_data = self.x_data.cuda()\n",
    "                            self.y_data = self.y_data.cuda()\n",
    "                    def __getitem__(self, index):\n",
    "                        return self.x_data[index], self.y_data[index]\n",
    "                    def __len__(self):\n",
    "                        return self.len\n",
    "\n",
    "                ## DEFINISCO I DATASET DA CARICARE    \n",
    "                traindataset = Traindataset()    # Train dataset\n",
    "                testdataset = Testdataset_L()    # Test dataset LEARNED\n",
    "                testdataset_U = Testdataset_U()    # Test dataset UNLEARNED\n",
    "\n",
    "                header( model_lst, k, i, traindataset, testdataset, testdataset_U)\n",
    "\n",
    "                #train_sampler,dev_sampler,test_sampler=dev_shuffle(shuffle_train,shuffle_test,val_split,traindataset,testdataset)\n",
    "                #train_sampler,dev_sampler,test_val_sampler,test_sampler=data_split(shuffle_train,shuffle_test,val_split,test_val_split,traindataset,testdataset)\n",
    "                \n",
    "                ## CREO I SAMPLER PER DIVIDERE IL TRAINDASTASET\n",
    "                tr_sampler, d_sampler = split_train(val_split, traindataset)\n",
    "                \n",
    "                ## LOADERS\n",
    "                # torch.utils.data.DataLoader deve avere come argomenti:\n",
    "                    # dataset -> quello che dobbiamo caricare\n",
    "                    # batch_size -> 'batch_size' definito negli hyperparameter della rete\n",
    "                    # drop_last = True -> Se la lunghezza del dataset non è divisibile per 'batch_size', \n",
    "                    # l'ultimo batch viene eliminato\n",
    "                # 'dev_loader e 'train_loader' hanno bisogno anche di\n",
    "                    # sampler = dev_sampler (una certa parte del trainset \n",
    "                    # deve essere usata come 'dev_set')\n",
    "                \n",
    "                # Training Set\n",
    "                train_loader = torch.utils.data.DataLoader(traindataset, batch_size = batch_size, \n",
    "                                                          sampler = tr_sampler, drop_last = True)\n",
    "                print('Train Set dimension: ' + str(len(train_loader)))\n",
    "                # Development Set\n",
    "                dev_loader = torch.utils.data.DataLoader(traindataset, batch_size=batch_size, \n",
    "                                                           sampler = d_sampler, drop_last=True)\n",
    "                print('Dev Set dimension: ' + str(len(dev_loader)))\n",
    "                # Testset LEARNED\n",
    "                test_val_loader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size,\n",
    "                                                                drop_last=True)\n",
    "                print('Test Set (L) dimension: ' + str(len(test_val_loader)))\n",
    "                # Testset UNLEARNED\n",
    "                test_loader = torch.utils.data.DataLoader(testdataset_U, batch_size = batch_size, \n",
    "                                                          drop_last = True)\n",
    "                print('Test Set (U) dimension: ' + str(len(test_loader)))\n",
    "                \n",
    "                modelClass = \"Model\" + str(k)\n",
    "                model = eval(modelClass)()\n",
    "                \n",
    "                if (use_cuda):\n",
    "                    model = model.cuda()\n",
    "\n",
    "                if doTrain:\n",
    "                    \n",
    "                    # criterion = nn.BCELoss(size_average=True)\n",
    "                    criterion = nn.BCELoss(reduction = 'mean')    # Cambiato perché dava un warning (size_average era un vecchio metodo in Pytorch)\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr)    \n",
    "                    msg = 'Accuracy on test set before training: '+str(accuracy(test_loader, model))+'\\n'\n",
    "                    print(msg)\n",
    "                    logfile.write(msg + \"\\n\")\n",
    "                    #EARLY STOP\n",
    "                    epoch = 0\n",
    "                    patience = 0\n",
    "                    best_acc_dev=0\n",
    "                    while (epoch<maxepoch and patience < maxpatience):\n",
    "                        running_loss = 0.0\n",
    "                        for l, data in enumerate(train_loader, 0):\n",
    "                            inputs, labels = data\n",
    "                            if use_cuda:\n",
    "                                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                            inputs, labels = Variable(inputs), Variable(labels)\n",
    "                            y_pred = model(inputs)\n",
    "                            if use_cuda:\n",
    "                                y_pred = y_pred.cuda()\n",
    "                            loss = criterion(y_pred, labels)\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            running_loss += loss.item()\n",
    "                            #print accuracy ever l mini-batches\n",
    "                            if l % 2000 == 1999:\n",
    "                                msg = '[%d, %5d] loss: %.3f' %(epoch + 1, l + 1, running_loss / 999)\n",
    "                                print(msg)\n",
    "                                logfile.write(msg + \"\\n\")\n",
    "                                running_loss = 0.0\n",
    "                                #msg = 'Accuracy on dev set:' + str(accuracy(dev_loader))\n",
    "                                #print(msg)\n",
    "                                #logfile.write(msg + \"\\n\")        \n",
    "                        accdev = (accuracy(dev_loader, model))\n",
    "                        msg = 'Accuracy on dev set:' + str(accdev)\n",
    "                        print(msg)\n",
    "                        logfile.write(msg + \"\\n\")        \n",
    "                        is_best = bool(accdev > best_acc_dev)\n",
    "                        best_acc_dev = (max(accdev, best_acc_dev))\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_acc_dev': best_acc_dev\n",
    "                        }, is_best,os.path.join(folder,'F'+str(i)+'best.pth.tar'), logfile)\n",
    "                        if is_best:\n",
    "                            patience=0\n",
    "                        else:\n",
    "                            patience = patience+1\n",
    "                        epoch = epoch+1\n",
    "                        logfile.flush()\n",
    "                        \n",
    "                if doEval:\n",
    "                    if use_cuda:                        \n",
    "                        state = torch.load(os.path.join(folder,'F'+str(i)+'best.pth.tar'))\n",
    "                    else:\n",
    "                        state = torch.load(os.path.join(folder,'F'+str(i)+'best.pth.tar'), map_location=lambda storage, loc: storage)\n",
    "                    stop_epoch = state['epoch']\n",
    "                    model.load_state_dict(state['state_dict'])\n",
    "                    if not use_cuda:\n",
    "                        model.cpu()\n",
    "                    accuracy_dev = state['best_acc_dev']\n",
    "                    model.eval()\n",
    "                    \n",
    "                    acctest = (accuracy(test_loader, model))\n",
    "                    acctest_val = (accuracy(test_val_loader, model))\n",
    "                    accs[i-1] = acctest\n",
    "                    accs_test_val[i-1] = acctest_val\n",
    "                    \n",
    "                    precision_0_U,recall_0_U,f1_score_0_U = pre_rec(test_loader, model, 0.0)\n",
    "                    precisions_0_U[i-1] = precision_0_U\n",
    "                    recalls_0_U[i-1] = recall_0_U\n",
    "                    f1_scores_0_U[i-1] = f1_score_0_U\n",
    "                    \n",
    "                    precision_1_U,recall_1_U,f1_score_1_U = pre_rec(test_loader, model, 1.0)\n",
    "                    precisions_1_U[i-1] = precision_1_U\n",
    "                    recalls_1_U[i-1] = recall_1_U\n",
    "                    f1_scores_1_U[i-1] = f1_score_1_U\n",
    "                    \n",
    "                    precision_0_L,recall_0_L,f1_score_0_L = pre_rec(test_val_loader, model, 0.0)\n",
    "                    precisions_0_L[i-1] = precision_0_L\n",
    "                    recalls_0_L[i-1] = recall_0_L\n",
    "                    f1_scores_0_L[i-1] = f1_score_0_L\n",
    "                    \n",
    "                    precision_1_L,recall_1_L,f1_score_1_L = pre_rec(test_val_loader, model, 1.0)\n",
    "                    precisions_1_L[i-1] = precision_1_L\n",
    "                    recalls_1_L[i-1] = recall_1_L\n",
    "                    f1_scores_1_L[i-1] = f1_score_1_L\n",
    "                    \n",
    "                    accs_dev[i-1] = accuracy_dev\n",
    "                    \n",
    "                    writer.writerow({'Fold': i,'Acc_L': acctest_val, 'Acc_U': acctest,\n",
    "                                     #'P_0_U': precision_0_U,'R_0_U': recall_0_U,'F1_0_U': f1_score_0_U,\n",
    "                                     'R_0_U': recall_0_U,\n",
    "                                     #'P_1_U': precision_1_U,'R_1_U': recall_1_U,'F1_1_U': f1_score_1_U,\n",
    "                                     'R_1_U': recall_1_U,\n",
    "                                     #'P_0_L': precision_0_L,'R_0_L': recall_0_L,'F1_0_L': f1_score_0_L,\n",
    "                                     'R_0_L': recall_0_L,\n",
    "                                     #'P_1_L': precision_1_L,'R_1_L': recall_1_L,'F1_1_L': f1_score_1_L,\n",
    "                                     'R_1_L': recall_1_L,\n",
    "                                     'Stop_epoch': stop_epoch,'Accuracy_dev': accuracy_dev})\n",
    "                    table.column_headers = fieldnames\n",
    "                    table.append_row([i,acctest_val,acctest,\n",
    "                                      #precision_0_U,recall_0_U,f1_score_0_U,\n",
    "                                      recall_0_U,\n",
    "                                      #precision_1_U,recall_1_U,f1_score_1_U,\n",
    "                                      recall_1_U,\n",
    "                                      #precision_0_L,recall_0_L,f1_score_0_L,\n",
    "                                      recall_0_L,\n",
    "                                      #precision_1_L,recall_1_L,f1_score_1_L,\n",
    "                                      recall_1_L,\n",
    "                                      stop_epoch,accuracy_dev])\n",
    "                    print(table)\n",
    "                    print('----------------------------------------------------------------------')\n",
    "                    logfile.write(str(table) + \"\\n----------------------------------------------------------------------\\n\")\n",
    "                    t1 = time.time()\n",
    "                    times[i-1] = int(t1-t0)\n",
    "                    \n",
    "                    ## -- Estrai il Basografico\n",
    "                \n",
    "                fold_predicted = fold_offset\n",
    "                if doExtractBaso:\n",
    "                    if use_cuda:                        \n",
    "                        state = torch.load(os.path.join(folder,'F'+str(fold_predicted)+'best.pth.tar'))\n",
    "                    else:\n",
    "                        state = torch.load(os.path.join(folder,'F'+str(fold_predicted)+'best.pth.tar'), map_location=lambda storage, loc: storage)\n",
    "                    stop_epoch = state['epoch']\n",
    "                    model.load_state_dict(state['state_dict'])\n",
    "                    if not use_cuda:\n",
    "                        model.cpu()\n",
    "                    accuracy_dev = state['best_acc_dev']\n",
    "                    model.eval()\n",
    "                    \n",
    "                    ## -- QUESTO DEVI CONTROLLARLO! SE ESCE CHE i È DIVERSO DALLA FOLD DEVI RIFARE TUTTO!\n",
    "                    print('F'+str(fold_predicted)+'best.pth.tar')\n",
    "                    predicted_baso = save_predicted_baso(test_loader, model, subject_predict, model_lst[k], fold_predicted)   \n",
    "            \n",
    "            duration = str(datetime.timedelta(seconds=np.sum(times)))\n",
    "            writer.writerow({})\n",
    "            writer.writerow({'Fold': 'Elapsed time: '+duration})\n",
    "            avg_acc_test_val = round(np.average(accs_test_val),3)\n",
    "            std_acc_test_val = round(np.std(accs_test_val),3)\n",
    "            \n",
    "            avg_acc_test_val,avg_a,avg_p_0_U,avg_r_0_U,avg_f_0_U,avg_p_1_U,avg_r_1_U,avg_f_1_U,avg_p_0_L,avg_r_0_L,avg_f_0_L,avg_p_1_L,avg_r_1_L,avg_f_1_L,avg_a_d=averages([accs_test_val,accs,precisions_0_U,recalls_0_U,f1_scores_0_U,precisions_1_U,recalls_1_U,f1_scores_1_U,precisions_0_L,recalls_0_L,f1_scores_0_L,precisions_1_L,recalls_1_L,f1_scores_1_L,accs_dev])\n",
    "            std_acc_test_val,std_a,std_p_0_U,std_r_0_U,std_f_0_U,std_p_1_U,std_r_1_U,std_f_1_U,std_p_0_L,std_r_0_L,std_f_0_L,std_p_1_L,std_r_1_L,std_f_1_L,std_a_d=stds([accs_test_val,accs,precisions_0_U,recalls_0_U,f1_scores_0_U,precisions_1_U,recalls_1_U,f1_scores_1_U,precisions_0_L,recalls_0_L,f1_scores_0_L,precisions_1_L,recalls_1_L,f1_scores_1_L,accs_dev])\n",
    "            \n",
    "            writer1.writerow({model_lst[k]: 'Acc_U','Avg': avg_a,'Std_dev': std_acc_test_val})\n",
    "            writer1.writerow({model_lst[k]: 'Acc_L','Avg': avg_acc_test_val,'Std_dev': std_a})\n",
    "            writer1.writerow({model_lst[k]: 'P_0_U','Avg': avg_p_0_U ,'Std_dev': std_p_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'R_0_U','Avg': avg_r_0_U,'Std_dev': std_r_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'F1_0_U','Avg': avg_f_0_U,'Std_dev': std_f_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'P_1_U','Avg': avg_p_1_U,'Std_dev': std_p_1_U})\n",
    "            writer1.writerow({model_lst[k]: 'R_1_U','Avg': avg_r_1_U,'Std_dev': std_r_1_U})\n",
    "            writer1.writerow({model_lst[k]: 'F1_1_U','Avg': avg_f_1_U,'Std_dev': std_f_1_U})            \n",
    "            writer1.writerow({model_lst[k]: 'P_0_L','Avg': avg_p_0_L,'Std_dev': std_p_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'R_0_L','Avg': avg_r_0_L,'Std_dev': std_r_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'F1_0_L','Avg': avg_f_0_L,'Std_dev': std_f_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'P_1_L','Avg': avg_p_1_L,'Std_dev': std_p_1_L})\n",
    "            writer1.writerow({model_lst[k]: 'R_1_L','Avg': avg_r_1_L,'Std_dev': std_r_1_L})\n",
    "            writer1.writerow({model_lst[k]: 'F1_1_L','Avg': avg_f_1_L,'Std_dev': std_f_1_L})                        \n",
    "            writer1.writerow({model_lst[k]: 'Acc_dev','Avg': avg_a_d,'Std_dev': std_a_d})\n",
    "            writer1.writerow({})\n",
    "            writer1.writerow({model_lst[k]: 'Elapsed time: '+duration})\n",
    "            avgtable.column_headers = fieldnames1\n",
    "            avgtable.append_row(['Acc_U',avg_a,std_a])\n",
    "            avgtable.append_row(['Acc_L',avg_acc_test_val,std_acc_test_val])\n",
    "            avgtable.append_row(['P_0_U',avg_p_0_U,std_p_0_U])\n",
    "            avgtable.append_row(['R_0_U',avg_r_0_U,std_r_0_U])\n",
    "            avgtable.append_row(['F1_0_U',avg_f_0_U,std_f_0_U])\n",
    "            avgtable.append_row(['P_1_U',avg_p_1_U,std_p_1_U])\n",
    "            avgtable.append_row(['R_1_U',avg_r_1_U,std_r_1_U])\n",
    "            avgtable.append_row(['F1_1_U',avg_f_1_U,std_f_1_U])                        \n",
    "            avgtable.append_row(['P_0_L',avg_p_0_L,std_p_0_L])\n",
    "            avgtable.append_row(['R_0_L',avg_r_0_L,std_r_0_L])\n",
    "            avgtable.append_row(['F1_0_L',avg_f_0_L,std_f_0_L])\n",
    "            avgtable.append_row(['P_1_L',avg_p_1_L,std_p_1_L])\n",
    "            avgtable.append_row(['R_1_L',avg_r_1_L,std_r_1_L])\n",
    "            avgtable.append_row(['F1_1_L',avg_f_1_L,std_f_1_L])            \n",
    "            avgtable.append_row(['Accuracy_dev',avg_a_d,std_a_d])\n",
    "            print(avgtable)\n",
    "            logfile.write(str(avgtable) + \"\\n\")\n",
    "            msg = 'Elapsed time: '+ duration + '\\n\\n'\n",
    "            print(msg)\n",
    "            logfile.write(msg )\n",
    "\n",
    "        logfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am NOT using CUDA: SUCCESS!\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "MODEL: FF6\n",
      "Fold: 1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "Trainset fold0 shape: 458180x41\n",
      "Testset (L) fold0 shape: 50943x41\n",
      "Testset (U) fold0 shape: 13583x41\n",
      "\n",
      "Train Set dimension: 11454\n",
      "Dev Set dimension: 2863\n",
      "Test Set (L) dimension: 1591\n",
      "Test Set (U) dimension: 424\n",
      "F23best.pth.tar\n",
      "predicting baso for subject 26 ... \n",
      "NN predicitons on sliding wondows: \n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Number of sliding windows: 13568\n",
      "Number of samples in a window: 200\n",
      "Number of samples: 271540\n",
      "\n",
      "Samples predictions:\n",
      "[ 1  1  1 ... -1 -1 -1]\n",
      "+--------------+------------------------+---------+\n",
      "|     FF6      |          Avg           | Std_dev |\n",
      "+--------------+------------------------+---------+\n",
      "|    Acc_U     |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    Acc_L     |          nan           |   nan   |\n",
      "+--------------+------------------------+---------+\n",
      "|    P_0_U     | 2.315841784746324e+77  |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    R_0_U     |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    F1_0_U    | 7.254882996355924e+169 |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    P_1_U     |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    R_1_U     |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    F1_1_U    |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    P_0_L     |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    R_0_L     | 2.315841784746324e+77  |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    F1_0_L    |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    P_1_L     |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    R_1_L     | 2.315841784746324e+77  |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "|    F1_1_L    |          0.0           |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "| Accuracy_dev | 2.315841784746324e+77  |   0.0   |\n",
      "+--------------+------------------------+---------+\n",
      "Elapsed time: 0:00:00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmuscles=int((len(traindata.columns)-1)/spw)\n",
    "if use_cuda and not use_gputil and cuda_device!=None and torch.cuda.is_available():\n",
    "    with torch.cuda.device(cuda_device):\n",
    "        print('I am using CUDA: SUCCESS!')\n",
    "        train_test()\n",
    "else:\n",
    "    print('I am NOT using CUDA: SUCCESS!')\n",
    "    train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
