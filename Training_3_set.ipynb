{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from torch import backends\n",
    "from beautifultable import BeautifulTable\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##SETTINGS\n",
    "doTrain = True\n",
    "doEval = True\n",
    "\n",
    "nfold = 5 #number of folds to train\n",
    "fold_offset = 1\n",
    "lr=0.01 #learning rate\n",
    "\n",
    "batch_size = 32\n",
    "val_split = .1 #trainset percentage allocated for devset\n",
    "test_val_split = .1 #trainset percentage allocated for test_val set (i.e. the test set of known patients)\n",
    "\n",
    "#cwd = os.getcwd()\n",
    "cwd = \"../subjects/min-max/windows_20/tr-True_sliding_20_c-False/folds_test\"\n",
    "subject = 1 # serve per caricare le folds da cartelle diverse\n",
    "prefix_train = 'TrainFold'\n",
    "prefix_test = 'TestFold'\n",
    "\n",
    "spw=20 #samples per window\n",
    "nmuscles=12 #initial number of muscles acquired\n",
    "\n",
    "#Enable/Disable shuffle on trainset/testset\n",
    "shuffle_train = False\n",
    "shuffle_test= False\n",
    "\n",
    "#Delete electrogonio signals\n",
    "# 3 = Left Rectus Femuris; 5 = Left Goniometer; 9 = Right Rectus Femuris; 11 = Right Goniometer\n",
    "exclude_features = True\n",
    "#Only use electrogonio signals\n",
    "include_only_features = False\n",
    "#Features to selected/deselected for input to the networks\n",
    "features_select = [3,5,9,11] #1 to 4\n",
    "\n",
    "#Select which models to run. Insert comma separated values into 'model_select' var.\n",
    "#List. 0:'FF', 1:'FC2', 2:'FC2DP', 3:'FC3', 4:'FC3dp', 5:'Conv1d', 6:'MultiConv1d' \n",
    "#e.g: model_select = [0,4,6] to select FF,FC3dp,MultiConv1d\n",
    "\n",
    "# Modelli del paper: 11 (FF2), 14 (FF4), 16 (FF5) --> Prova questi!\n",
    "# FF6 per testarlo potente dopo (17)\n",
    "model_lst = ['FF','FC2','FC2DP','FC3','FC3dp','Conv1d','MultiConv1d',\n",
    "             'MultiConv1d_2','MultiConv1d_3', 'MultiConv1d_4', 'MultiConv1d_5', \n",
    "             'FF2', 'CNN1', 'FF3', 'FF4', 'CNN2', 'FF5', 'FF6', 'CNN3', 'CNN1-FF5', 'CNN1-2','CNN1-1', 'CNN1-3', 'CNN_w60']\n",
    "model_select = [11] \n",
    "\n",
    "#Early stop settings\n",
    "maxepoch = 100\n",
    "maxpatience = 10\n",
    "\n",
    "use_cuda = False\n",
    "use_gputil = False\n",
    "cuda_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CUDA\n",
    "\n",
    "if use_gputil and torch.cuda.is_available():\n",
    "    import GPUtil\n",
    "\n",
    "    # Get the first available GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    try:\n",
    "        deviceIDs = GPUtil.getAvailable(order='memory', limit=1, maxLoad=100, maxMemory=20)  # return a list of available gpus\n",
    "    except:\n",
    "        print('GPU not compatible with NVIDIA-SMI')\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(deviceIDs[0])\n",
    "        \n",
    "    ttens = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "    ttens = ttens.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? --> False\n",
      "Cuda Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print('Is CUDA available? --> ' + str(torch.cuda.is_available()))\n",
    "print('Cuda Device: ' + str(cuda_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seeds\n",
    "def setSeeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "setSeeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prints header of beautifultable report for each fold\n",
    "def header(model_list,nmodel,nfold,traindataset,testdataset):\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('MODEL: '+model_list[nmodel])\n",
    "    print('Fold: '+str(nfold))\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++\\n\\n')\n",
    "    shape = list(traindataset.x_data.shape)\n",
    "    print('Trainset fold'+str(i)+' shape: '+str(shape[0])+'x'+str((shape[1]+1)))\n",
    "    shape = list(testdataset.x_data.shape)\n",
    "    print('Testset fold'+str(i)+' shape: '+str(shape[0])+'x'+str((shape[1]+1))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prints actual beautifultable for each fold\n",
    "def table(model_list,nmodel,accuracies,precisions,recalls,f1_scores,accuracies_dev):\n",
    "    table = BeautifulTable()\n",
    "    table.column_headers = [\"{}\".format(model_list[nmodel]), \"Avg\", \"Stdev\"]\n",
    "    table.append_row([\"Accuracy\",round(np.average(accuracies),3),round(np.std(accuracies),3)])\n",
    "    table.append_row([\"Precision\",round(np.average(precisions),3),round(np.std(precisions),3)])\n",
    "    table.append_row([\"Recall\",round(np.average(recalls),3),round(np.std(recalls),3)])\n",
    "    table.append_row([\"F1_score\",round(np.average(f1_scores),3),round(np.std(f1_scores),3)])\n",
    "    table.append_row([\"Accuracy_dev\",round(np.average(accuracies_dev),3),round(np.std(accuracies_dev),3)])    \n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saves best model state on disk for each fold\n",
    "def save_checkpoint (state, is_best, filename, logfile):\n",
    "    if is_best:\n",
    "        msg = \"=> Saving a new best. \"+'Epoch: '+str(state['epoch'])\n",
    "        print (msg)\n",
    "        logfile.write(msg + \"\\n\")\n",
    "        torch.save(state, filename)  \n",
    "    else:\n",
    "        msg = \"=> Validation accuracy did not improve. \"+'Epoch: '+str(state['epoch'])\n",
    "        print (msg)\n",
    "        logfile.write(msg + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute sklearn metrics: Recall, Precision, F1-score\n",
    "def pre_rec (loader, model, positiveLabel):\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate (loader,0):\n",
    "            inputs, labels = data\n",
    "            y_true = np.append(y_true,labels.cpu())\n",
    "            outputs = model(inputs)\n",
    "            outputs[outputs>=0.5] = 1\n",
    "            outputs[outputs<0.5] = 0\n",
    "            y_pred = np.append(y_pred,outputs.cpu())\n",
    "    y_true = np.where(y_true==positiveLabel,0,1)\n",
    "    y_pred = np.where(y_pred==positiveLabel,0,1)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    return round(precision*100,3), round(recall*100,3), round(f1_score*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates model accuracy. Predicted vs Correct.\n",
    "def accuracy (loader, model):\n",
    "    total=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            outputs[outputs>=0.5] = 1\n",
    "            outputs[outputs<0.5] = 0\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "    return round((100 * correct / total),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Arrays to store metrics\n",
    "accs = np.empty([nfold,1])\n",
    "accs_test_val = np.empty([nfold,1])\n",
    "precisions_0_U = np.empty([nfold,1])\n",
    "recalls_0_U = np.empty([nfold,1])\n",
    "f1_scores_0_U = np.empty([nfold,1])\n",
    "precisions_1_U = np.empty([nfold,1])\n",
    "recalls_1_U = np.empty([nfold,1])\n",
    "f1_scores_1_U = np.empty([nfold,1])\n",
    "precisions_0_L = np.empty([nfold,1])\n",
    "recalls_0_L = np.empty([nfold,1])\n",
    "f1_scores_0_L = np.empty([nfold,1])\n",
    "precisions_1_L = np.empty([nfold,1])\n",
    "recalls_1_L = np.empty([nfold,1])\n",
    "f1_scores_1_L = np.empty([nfold,1])\n",
    "accs_dev = np.empty([nfold,1])\n",
    "times = np.empty([nfold,1])\n",
    "\n",
    "#Calculate avg metrics on folds\n",
    "def averages (vals):\n",
    "    avgs = []\n",
    "    for val in vals:\n",
    "        avgs.append(round(np.average(val),3))\n",
    "    return avgs\n",
    "\n",
    "#Calculate std metrics on folds\n",
    "def stds (vals):\n",
    "    stds = []\n",
    "    for val in vals:\n",
    "        stds.append(round(np.std(val),3))\n",
    "    return stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle\n",
    "def dev_shuffle (shuffle_train,shuffle_test,val_split,traindataset,testdataset):\n",
    "    train_size = len(traindataset)\n",
    "    test_size = len(testdataset)\n",
    "    train_indices = list(range(train_size))\n",
    "    test_indices = list(range(test_size))\n",
    "    split = int(np.floor(val_split * train_size))\n",
    "    if shuffle_train:\n",
    "        np.random.shuffle(train_indices)\n",
    "    if shuffle_test:\n",
    "        np.random.shuffle(test_indices) \n",
    "    train_indices, dev_indices = train_indices[split:], train_indices[:split]\n",
    "    # Samplers\n",
    "    tr_sampler = SubsetRandomSampler(train_indices)\n",
    "    d_sampler = SubsetRandomSampler(dev_indices)\n",
    "    te_sampler = SubsetRandomSampler(test_indices)\n",
    "    return tr_sampler,d_sampler,te_sampler\n",
    "\n",
    "def data_split (shuffle_train,shuffle_test,val_split,test_val_split,traindataset,testdataset):\n",
    "    train_size = len(traindataset)\n",
    "    test_size = len(testdataset)\n",
    "    train_indices = list(range(train_size))\n",
    "    test_indices = list(range(test_size))\n",
    "    test_val_split = int(np.floor(test_val_split * train_size)) \n",
    "    dev_split = int(np.floor(val_split * (train_size-test_val_split) + test_val_split))\n",
    "    if shuffle_train:\n",
    "        np.random.shuffle(train_indices)\n",
    "    if shuffle_test:\n",
    "        np.random.shuffle(test_indices) \n",
    "    train_indices, dev_indices, test_val_indices = train_indices[dev_split:], train_indices[test_val_split:dev_split], train_indices[:test_val_split]\n",
    "    # Samplers\n",
    "    tr_sampler = SubsetRandomSampler(train_indices)\n",
    "    d_sampler = SubsetRandomSampler(dev_indices)\n",
    "    tv_sampler = SubsetRandomSampler(test_val_indices)                \n",
    "    te_sampler = SubsetRandomSampler(test_indices)\n",
    "    return tr_sampler,d_sampler,tv_sampler,te_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 1\n",
      "Train dataset: 867073 windows;\n",
      "Test dataset (LEARNED): 96389 windows;\n",
      "Test dataset (UNLEARNED) 257150 windows.\n",
      "Loading fold 2\n",
      "Train dataset: 895932 windows;\n",
      "Test dataset (LEARNED): 99592 windows;\n",
      "Test dataset (UNLEARNED) 225088 windows.\n",
      "Loading fold 3\n",
      "Train dataset: 869368 windows;\n",
      "Test dataset (LEARNED): 96642 windows;\n",
      "Test dataset (UNLEARNED) 254602 windows.\n",
      "Loading fold 4\n",
      "Train dataset: 885991 windows;\n",
      "Test dataset (LEARNED): 98489 windows;\n",
      "Test dataset (UNLEARNED) 236132 windows.\n",
      "Loading fold 5\n",
      "Train dataset: 881308 windows;\n",
      "Test dataset (LEARNED): 97968 windows;\n",
      "Test dataset (UNLEARNED) 241336 windows.\n",
      "Loading fold 6\n",
      "Train dataset: 863340 windows;\n",
      "Test dataset (LEARNED): 95973 windows;\n",
      "Test dataset (UNLEARNED) 261299 windows.\n",
      "Loading fold 7\n"
     ]
    }
   ],
   "source": [
    "#Loads and appends all folds all at once\n",
    "trainfolds = []    # Train set\n",
    "testfolds = []    # Test set (LEARNED)\n",
    "testfolds_U = []    # Test set (UNLEARNED)\n",
    "\n",
    "col_select = np.array([])\n",
    "\n",
    "#This is an hack to test smaller windows\n",
    "for i in range (spw*nmuscles,200):\n",
    "    col_select = np.append(col_select,i)\n",
    "    \n",
    "for i in range (0,spw*nmuscles,nmuscles):\n",
    "    for muscle in features_select:\n",
    "        col_select = np.append(col_select,muscle -1 + i)\n",
    "    cols=np.arange(0,spw*nmuscles+1)\n",
    "\n",
    "if exclude_features & (not include_only_features): #delete gonio\n",
    "    for j in range(fold_offset,fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_csv(os.path.join(cwd, prefix_train + '_' + str(j)+'.csv'),sep=',',header=None,dtype=np.float32,usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        trainfolds.append(traindata)\n",
    "        testdata = pd.read_csv(os.path.join(cwd, prefix_test + '_L_' + str(j)+'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        testfolds.append(testdata) \n",
    "        testdata_U = pd.read_csv(os.path.join(cwd, prefix_test + '_U_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        testfolds_U.append(testdata_U)    # Aggiunto testfold UNLEARNED \n",
    "        print('Train dataset: ' + str(len(traindata)) + ' windows;')\n",
    "        print('Test dataset (LEARNED): ' + str(len(testdata)) + ' windows;')\n",
    "        print('Test dataset (UNLEARNED) ' + str(len(testdata_U)) + ' windows.')\n",
    "        \n",
    "elif include_only_features & (not exclude_features): #only gonio\n",
    "    for j in range(fold_offset, fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_table(os.path.join(cwd, prefix_train + str(j)+'.csv'),sep=',',header=None,dtype=np.float32,usecols=[i for i in cols if i in col_select.astype(int)])\n",
    "        testdata = pd.read_table(os.path.join(cwd, prefix_test + str(j)+'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i in col_select.astype(int)])\n",
    "        trainfolds.append(traindata)\n",
    "        testfolds.append(testdata) \n",
    "elif (not include_only_features) & (not exclude_features): \n",
    "    for j in range(fold_offset,fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_csv(os.path.join(cwd, prefix_train + '_' + str(j) + '.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        testdata = pd.read_csv(os.path.join(cwd, prefix_test + '_L_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        testdata_U = pd.read_csv(os.path.join(cwd, prefix_test + '_U_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        trainfolds.append(traindata)\n",
    "        testfolds.append(testdata)\n",
    "        testfolds_U.append(testdata_U)    # Aggiunto testfold UNLEARNED \n",
    "        print('Train dataset: ' + str(len(traindata)) + ' windows;')\n",
    "        print('Test dataset (LEARNED): ' + str(len(testdata)) + ' windows;')\n",
    "        print('Test dataset (UNLEARNED) ' + str(len(testdata_U)) + ' windows.')\n",
    "else:\n",
    "    raise ValueError('use_gonio and del_gonio cannot be both True')\n",
    "\n",
    "nmuscles=int((len(traindata.columns)-1)/spw)\n",
    "print('\\nEach window is composed by ' + str(len(traindata.columns)) + ' samples.')\n",
    "print('The number of muscles is ' + str(nmuscles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmuscles=int((len(traindata.columns)-1)/spw) #used for layer dimensions and stride CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models\n",
    "from models import *\n",
    "models._spw = spw\n",
    "models._nmuscles = nmuscles\n",
    "models._batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(models._nmuscles)\n",
    "\n",
    "#import models\n",
    "#from models import *\n",
    "#TEST DIMENSIONS\n",
    "#models.nmuscles = nmuscles\n",
    "def testdimensions():\n",
    "    model = Model23()\n",
    "    print(model)\n",
    "    x = torch.randn(32,1,480)\n",
    "    model.test_dim(x)\n",
    " \n",
    "#testdimensions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fieldnames = ['Fold','Acc_L', 'Acc_U',\n",
    "              'R_0_U','R_1_U',\n",
    "              'R_0_L','R_1_L',\n",
    "              'Stop_epoch','Accuracy_dev'] #coloumn names report FOLD CSV\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#TRAINING LOOP\n",
    "def train_test():\n",
    "    for k in model_select:\n",
    "        \n",
    "        table = BeautifulTable()\n",
    "        avgtable = BeautifulTable()\n",
    "        fieldnames1 = [model_lst[k],'Avg','Std_dev'] #column names report GLOBAL CSV\n",
    "        folder = os.path.join(cwd,'Report_'+str(model_lst[k]))\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        logfilepath = os.path.join(folder,'log.txt')\n",
    "        logfile = open(logfilepath,\"w\") \n",
    "\n",
    "        with open(os.path.join(folder,'Report_folds.csv'),'w') as f_fold, open(os.path.join(folder,'Report_global.csv'),'w') as f_global:\n",
    "            writer = csv.DictWriter(f_fold, fieldnames = fieldnames)\n",
    "            writer1  = csv.DictWriter(f_global, fieldnames = fieldnames1)\n",
    "            writer.writeheader()\n",
    "            writer1.writeheader()\n",
    "            t0 = 0\n",
    "            t1 = 0\n",
    "            for i in range(1,nfold+1):\n",
    "                \n",
    "                t0 = time.time()\n",
    "                setSeeds(0)\n",
    "                \n",
    "                class Traindataset(Dataset):\n",
    "                    def __init__(self):\n",
    "                        self.data=trainfolds[i-1]\n",
    "                        self.x_data=torch.from_numpy(np.asarray(self.data.iloc[:, 0:-1])) \n",
    "                        self.len=self.data.shape[0]\n",
    "                        self.y_data = torch.from_numpy(np.asarray(self.data.iloc[:, [-1]]))\n",
    "                        if (use_cuda):\n",
    "                            self.x_data = self.x_data.cuda()\n",
    "                            self.y_data = self.y_data.cuda()\n",
    "                    def __getitem__(self, index):\n",
    "                        return self.x_data[index], self.y_data[index]\n",
    "                    def __len__(self):\n",
    "                        return self.len\n",
    "                class Testdataset(Dataset):\n",
    "                    def __init__(self):\n",
    "                        self.data=testfolds[i-1]\n",
    "                        self.x_data=torch.from_numpy(np.asarray(self.data.iloc[:, 0:-1]))\n",
    "                        self.len=self.data.shape[0]\n",
    "                        self.y_data = torch.from_numpy(np.asarray(self.data.iloc[:, [-1]]))\n",
    "                        if (use_cuda):\n",
    "                            self.x_data = self.x_data.cuda()\n",
    "                            self.y_data = self.y_data.cuda()\n",
    "                    def __getitem__(self, index):\n",
    "                        return self.x_data[index], self.y_data[index]\n",
    "                    def __len__(self):\n",
    "                        return self.len\n",
    "\n",
    "                traindataset = Traindataset()\n",
    "                testdataset = Testdataset()\n",
    "                testdataset_U = Testdataset()    # Aggiunto, relativo agli UNLEARNED\n",
    "\n",
    "                header(model_lst,k,i,traindataset,testdataset)\n",
    "\n",
    "                #train_sampler,dev_sampler,test_sampler=dev_shuffle(shuffle_train,shuffle_test,val_split,traindataset,testdataset)\n",
    "                train_sampler,dev_sampler,test_val_sampler,test_sampler=data_split(shuffle_train,shuffle_test,val_split,test_val_split,traindataset,testdataset)\n",
    "                \n",
    "                #loaders\n",
    "                train_loader = torch.utils.data.DataLoader(traindataset, batch_size=batch_size, \n",
    "                                                           sampler=train_sampler,drop_last=True)\n",
    "                test_val_loader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size,\n",
    "                                                                sampler=test_val_sampler,drop_last=True)\n",
    "                dev_loader = torch.utils.data.DataLoader(traindataset, batch_size=batch_size, \n",
    "                                                           sampler=dev_sampler,drop_last=True)\n",
    "                \n",
    "                # Questo l'ho cambiato da 'testdataset' a 'testdataset_U'\n",
    "                test_loader = torch.utils.data.DataLoader(testdataset_U, batch_size=batch_size,\n",
    "                                                                sampler=test_sampler,drop_last=True)\n",
    "                modelClass = \"Model\" + str(k)\n",
    "                model = eval(modelClass)()\n",
    "                \n",
    "                if (use_cuda):\n",
    "                    model = model.cuda()\n",
    "\n",
    "                if doTrain:\n",
    "                    \n",
    "                    # criterion = nn.BCELoss(size_average=True)\n",
    "                    criterion = nn.BCELoss(reduction = 'mean')    # Cambiato perchÃ© dava un warning (size_average era un vecchio metodo in Pytorch)\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr)    \n",
    "                    msg = 'Accuracy on test set before training: '+str(accuracy(test_loader, model))+'\\n'\n",
    "                    print(msg)\n",
    "                    logfile.write(msg + \"\\n\")\n",
    "                    #EARLY STOP\n",
    "                    epoch = 0\n",
    "                    patience = 0\n",
    "                    best_acc_dev=0\n",
    "                    while (epoch<maxepoch and patience < maxpatience):\n",
    "                        running_loss = 0.0\n",
    "                        for l, data in enumerate(train_loader, 0):\n",
    "                            inputs, labels = data\n",
    "                            if use_cuda:\n",
    "                                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                            inputs, labels = Variable(inputs), Variable(labels)\n",
    "                            y_pred = model(inputs)\n",
    "                            if use_cuda:\n",
    "                                y_pred = y_pred.cuda()\n",
    "                            loss = criterion(y_pred, labels)\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            running_loss += loss.item()\n",
    "                            #print accuracy ever l mini-batches\n",
    "                            if l % 2000 == 1999:\n",
    "                                msg = '[%d, %5d] loss: %.3f' %(epoch + 1, l + 1, running_loss / 999)\n",
    "                                print(msg)\n",
    "                                logfile.write(msg + \"\\n\")\n",
    "                                running_loss = 0.0\n",
    "                                #msg = 'Accuracy on dev set:' + str(accuracy(dev_loader))\n",
    "                                #print(msg)\n",
    "                                #logfile.write(msg + \"\\n\")        \n",
    "                        accdev = (accuracy(dev_loader, model))\n",
    "                        msg = 'Accuracy on dev set:' + str(accdev)\n",
    "                        print(msg)\n",
    "                        logfile.write(msg + \"\\n\")        \n",
    "                        is_best = bool(accdev > best_acc_dev)\n",
    "                        best_acc_dev = (max(accdev, best_acc_dev))\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_acc_dev': best_acc_dev\n",
    "                        }, is_best,os.path.join(folder,'F'+str(i)+'best.pth.tar'), logfile)\n",
    "                        if is_best:\n",
    "                            patience=0\n",
    "                        else:\n",
    "                            patience = patience+1\n",
    "                        epoch = epoch+1\n",
    "                        logfile.flush()\n",
    "                        \n",
    "                if doEval:\n",
    "                    if use_cuda:                        \n",
    "                        state = torch.load(os.path.join(folder,'F'+str(i)+'best.pth.tar'))\n",
    "                    else:\n",
    "                        state = torch.load(os.path.join(folder,'F'+str(i)+'best.pth.tar'), map_location=lambda storage, loc: storage)\n",
    "                    stop_epoch = state['epoch']\n",
    "                    model.load_state_dict(state['state_dict'])\n",
    "                    if not use_cuda:\n",
    "                        model.cpu()\n",
    "                    accuracy_dev = state['best_acc_dev']\n",
    "                    model.eval()\n",
    "                    acctest = (accuracy(test_loader, model))\n",
    "                    acctest_val = (accuracy(test_val_loader, model))\n",
    "                    accs[i-1] = acctest\n",
    "                    accs_test_val[i-1] = acctest_val\n",
    "                    \n",
    "                    precision_0_U,recall_0_U,f1_score_0_U = pre_rec(test_loader, model, 0.0)\n",
    "                    precisions_0_U[i-1] = precision_0_U\n",
    "                    recalls_0_U[i-1] = recall_0_U\n",
    "                    f1_scores_0_U[i-1] = f1_score_0_U\n",
    "                    \n",
    "                    precision_1_U,recall_1_U,f1_score_1_U = pre_rec(test_loader, model, 1.0)\n",
    "                    precisions_1_U[i-1] = precision_1_U\n",
    "                    recalls_1_U[i-1] = recall_1_U\n",
    "                    f1_scores_1_U[i-1] = f1_score_1_U\n",
    "                    \n",
    "                    precision_0_L,recall_0_L,f1_score_0_L = pre_rec(test_val_loader, model, 0.0)\n",
    "                    precisions_0_L[i-1] = precision_0_L\n",
    "                    recalls_0_L[i-1] = recall_0_L\n",
    "                    f1_scores_0_L[i-1] = f1_score_0_L\n",
    "                    \n",
    "                    precision_1_L,recall_1_L,f1_score_1_L = pre_rec(test_val_loader, model, 1.0)\n",
    "                    precisions_1_L[i-1] = precision_1_L\n",
    "                    recalls_1_L[i-1] = recall_1_L\n",
    "                    f1_scores_1_L[i-1] = f1_score_1_L\n",
    "                    \n",
    "                    accs_dev[i-1] = accuracy_dev\n",
    "                    \n",
    "                    writer.writerow({'Fold': i,'Acc_L': acctest_val, 'Acc_U': acctest,\n",
    "                                     #'P_0_U': precision_0_U,'R_0_U': recall_0_U,'F1_0_U': f1_score_0_U,\n",
    "                                     'R_0_U': recall_0_U,\n",
    "                                     #'P_1_U': precision_1_U,'R_1_U': recall_1_U,'F1_1_U': f1_score_1_U,\n",
    "                                     'R_1_U': recall_1_U,\n",
    "                                     #'P_0_L': precision_0_L,'R_0_L': recall_0_L,'F1_0_L': f1_score_0_L,\n",
    "                                     'R_0_L': recall_0_L,\n",
    "                                     #'P_1_L': precision_1_L,'R_1_L': recall_1_L,'F1_1_L': f1_score_1_L,\n",
    "                                     'R_1_L': recall_1_L,\n",
    "                                     'Stop_epoch': stop_epoch,'Accuracy_dev': accuracy_dev})\n",
    "                    table.column_headers = fieldnames\n",
    "                    table.append_row([i,acctest_val,acctest,\n",
    "                                      #precision_0_U,recall_0_U,f1_score_0_U,\n",
    "                                      recall_0_U,\n",
    "                                      #precision_1_U,recall_1_U,f1_score_1_U,\n",
    "                                      recall_1_U,\n",
    "                                      #precision_0_L,recall_0_L,f1_score_0_L,\n",
    "                                      recall_0_L,\n",
    "                                      #precision_1_L,recall_1_L,f1_score_1_L,\n",
    "                                      recall_1_L,\n",
    "                                      stop_epoch,accuracy_dev])\n",
    "                    print(table)\n",
    "                    print('----------------------------------------------------------------------')\n",
    "                    logfile.write(str(table) + \"\\n----------------------------------------------------------------------\\n\")\n",
    "                    t1 = time.time()\n",
    "                    times[i-1] = int(t1-t0)\n",
    "            \n",
    "            duration = str(datetime.timedelta(seconds=np.sum(times)))\n",
    "            writer.writerow({})\n",
    "            writer.writerow({'Fold': 'Elapsed time: '+duration})\n",
    "            avg_acc_test_val = round(np.average(accs_test_val),3)\n",
    "            std_acc_test_val = round(np.std(accs_test_val),3)\n",
    "            \n",
    "            avg_acc_test_val,avg_a,avg_p_0_U,avg_r_0_U,avg_f_0_U,avg_p_1_U,avg_r_1_U,avg_f_1_U,avg_p_0_L,avg_r_0_L,avg_f_0_L,avg_p_1_L,avg_r_1_L,avg_f_1_L,avg_a_d=averages([accs_test_val,accs,precisions_0_U,recalls_0_U,f1_scores_0_U,precisions_1_U,recalls_1_U,f1_scores_1_U,precisions_0_L,recalls_0_L,f1_scores_0_L,precisions_1_L,recalls_1_L,f1_scores_1_L,accs_dev])\n",
    "            std_acc_test_val,std_a,std_p_0_U,std_r_0_U,std_f_0_U,std_p_1_U,std_r_1_U,std_f_1_U,std_p_0_L,std_r_0_L,std_f_0_L,std_p_1_L,std_r_1_L,std_f_1_L,std_a_d=stds([accs_test_val,accs,precisions_0_U,recalls_0_U,f1_scores_0_U,precisions_1_U,recalls_1_U,f1_scores_1_U,precisions_0_L,recalls_0_L,f1_scores_0_L,precisions_1_L,recalls_1_L,f1_scores_1_L,accs_dev])\n",
    "            \n",
    "            writer1.writerow({model_lst[k]: 'Acc_U','Avg': avg_a,'Std_dev': std_acc_test_val})\n",
    "            writer1.writerow({model_lst[k]: 'Acc_L','Avg': avg_acc_test_val,'Std_dev': std_a})\n",
    "            writer1.writerow({model_lst[k]: 'P_0_U','Avg': avg_p_0_U ,'Std_dev': std_p_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'R_0_U','Avg': avg_r_0_U,'Std_dev': std_r_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'F1_0_U','Avg': avg_f_0_U,'Std_dev': std_f_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'P_1_U','Avg': avg_p_1_U,'Std_dev': std_p_1_U})\n",
    "            writer1.writerow({model_lst[k]: 'R_1_U','Avg': avg_r_1_U,'Std_dev': std_r_1_U})\n",
    "            writer1.writerow({model_lst[k]: 'F1_1_U','Avg': avg_f_1_U,'Std_dev': std_f_1_U})            \n",
    "            writer1.writerow({model_lst[k]: 'P_0_L','Avg': avg_p_0_L,'Std_dev': std_p_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'R_0_L','Avg': avg_r_0_L,'Std_dev': std_r_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'F1_0_L','Avg': avg_f_0_L,'Std_dev': std_f_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'P_1_L','Avg': avg_p_1_L,'Std_dev': std_p_1_L})\n",
    "            writer1.writerow({model_lst[k]: 'R_1_L','Avg': avg_r_1_L,'Std_dev': std_r_1_L})\n",
    "            writer1.writerow({model_lst[k]: 'F1_1_L','Avg': avg_f_1_L,'Std_dev': std_f_1_L})                        \n",
    "            writer1.writerow({model_lst[k]: 'Acc_dev','Avg': avg_a_d,'Std_dev': std_a_d})\n",
    "            writer1.writerow({})\n",
    "            writer1.writerow({model_lst[k]: 'Elapsed time: '+duration})\n",
    "            avgtable.column_headers = fieldnames1\n",
    "            avgtable.append_row(['Acc_U',avg_a,std_a])\n",
    "            avgtable.append_row(['Acc_L',avg_acc_test_val,std_acc_test_val])\n",
    "            avgtable.append_row(['P_0_U',avg_p_0_U,std_p_0_U])\n",
    "            avgtable.append_row(['R_0_U',avg_r_0_U,std_r_0_U])\n",
    "            avgtable.append_row(['F1_0_U',avg_f_0_U,std_f_0_U])\n",
    "            avgtable.append_row(['P_1_U',avg_p_1_U,std_p_1_U])\n",
    "            avgtable.append_row(['R_1_U',avg_r_1_U,std_r_1_U])\n",
    "            avgtable.append_row(['F1_1_U',avg_f_1_U,std_f_1_U])                        \n",
    "            avgtable.append_row(['P_0_L',avg_p_0_L,std_p_0_L])\n",
    "            avgtable.append_row(['R_0_L',avg_r_0_L,std_r_0_L])\n",
    "            avgtable.append_row(['F1_0_L',avg_f_0_L,std_f_0_L])\n",
    "            avgtable.append_row(['P_1_L',avg_p_1_L,std_p_1_L])\n",
    "            avgtable.append_row(['R_1_L',avg_r_1_L,std_r_1_L])\n",
    "            avgtable.append_row(['F1_1_L',avg_f_1_L,std_f_1_L])            \n",
    "            avgtable.append_row(['Accuracy_dev',avg_a_d,std_a_d])\n",
    "            print(avgtable)\n",
    "            logfile.write(str(avgtable) + \"\\n\")\n",
    "            msg = 'Elapsed time: '+ duration + '\\n\\n'\n",
    "            print(msg)\n",
    "            logfile.write(msg )\n",
    "\n",
    "        logfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmuscles=int((len(traindata.columns)-1)/spw)\n",
    "if use_cuda and not use_gputil and cuda_device!=None and torch.cuda.is_available():\n",
    "    with torch.cuda.device(cuda_device):\n",
    "        print('I am using CUDA: SUCCESS!')\n",
    "        train_test()\n",
    "else:\n",
    "    print('I am NOT using CUDA: SUCCESS!')\n",
    "    train_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
